{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport torch\nimport re\ncmap = plt.cm.get_cmap('coolwarm')\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Use for pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import cohen_kappa_score, f1_score\n\nfrom nltk.tokenize import word_tokenize\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier \nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-27T09:09:48.436529Z","iopub.execute_input":"2024-05-27T09:09:48.436957Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\ndf_test = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"essay_id_dropped = df_train['essay_id']\ndf_train = df_train.drop('essay_id', axis = 1)\ndf_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\n\ncList = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    # \"he'd\": \"he would\",  ## --> he had or he would\n    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n    # \"I'd\": \"I would\",   ## --> I had or I would\n    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n    # \"it'd\": \"it had\",   ## --> It had or It would\n    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n    # \"she'd\": \"she would\",   ## --> It had or It would\n    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\"so's\": \"so is\",\n    # \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n    # \"there'd\": \"there had\",\n    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n    # \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n    # \"we'd\": \"we had\",\n    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n    \"when's\": \"when is\",\"when've\": \"when have\",\n    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n    \"you're\": \"you are\",  \"you've\": \"you have\"\n}\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef remove_punctuation(text):\n    \"\"\"\n    Remove all punctuation from the input text.\n    \n    Args:\n    - text (str): The input text.\n    \n    Returns:\n    - str: The text with punctuation removed.\n    \"\"\"\n    # string.punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\ndef dataPreprocessing(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef dataPreprocessing_w_contract(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef dataPreprocessing_w_punct_remove(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = remove_punctuation(x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef dataPreprocessing_w_contract_punct_remove(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = remove_punctuation(x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of words\nimport string\ndef preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n    df['word_count'] = df['full_text'].apply(lambda x: len(x.split()))\n\n    # Length\n    df['essay_length'] = df['full_text'].str.len()\n\n    # Sentences count\n    # Adding a new column 'sentences_count' that counts the sentences in 'full_text'\n    df['sentences_count'] = df['full_text'].str.count(r'\\.')\n\n    # Paragraph count\n    # Adding a new column 'paragraph_count' that counts the paragraphs in 'full_text'\n    df['paragraph_count'] = df['full_text'].str.count(r'\\n') + 1\n    \n    df[\"text_tokens\"] = df[\"full_text\"].apply(lambda x: word_tokenize(x))\n    df[\"word_count\"] = df[\"text_tokens\"].apply(lambda x: len(x))\n    df[\"unique_word_count\"] = df[\"text_tokens\"].apply(lambda x: len(set(x)))\n\n#     df[\"processed_text\"] = df[\"full_text\"].apply(lambda x: dataPreprocessing(x))\n#     df[\"text_tokens\"] = df[\"processed_text\"].apply(lambda x: word_tokenize(x))\n#     df[\"text_length_p\"] = df[\"processed_text\"].apply(lambda x: len(x))\n#     df[\"word_count_p\"] = df[\"text_tokens\"].apply(lambda x: len(x))\n#     df[\"unique_word_count_p\"] = df[\"text_tokens\"].apply(lambda x: len(set(x)))\n    \n#     df.drop(columns=[\"processed_text\", \"text_tokens\"], inplace=True)\n    return df\n\ndef clean_text(text):\n    text = re.sub(r'\\n', ' ', text)  # Loại bỏ xuống dòng\n    text = re.sub(r'[^\\w\\s]', '', text)  # Loại bỏ ký tự đặc biệt\n    text = text.lower()  # Chuyển thành chữ thường\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = preprocess_df(df_train)\ndf_train['clean_text'] = df_train['full_text'].apply(clean_text)\ndf_train = df_train.drop('full_text', axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rows_to_drop = df_train.query('sentences_count > 60 | (sentences_count > 50 & score == 1)').index\n\n# # Xóa các dòng có index tương ứng\n\n# df_train.drop(rows_to_drop, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rows_to_drop = df_train.query('paragraph_count > 80 | (paragraph_count > 55 & score == 1 ) | (paragraph_count >60 & score == 2 ) | (paragraph_count > 30  & score == 5) | (paragraph_count > 25  & score == 6)').index\n\n# # Xóa các dòng có index tương ứng\n\n# df_train.drop(rows_to_drop, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decomment if want to use Pipeline again\n\n# Define transformers for numerical and categorical columns\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse = False))\n])\n\n# Add text vectorization step\ntext_vectorizer = TfidfVectorizer(\n    encoding='utf-8',\n    ngram_range=(1, 3),\n    strip_accents='unicode',\n    analyzer='word',\n    min_df=0.05,\n    max_df=0.95,\n    sublinear_tf=True\n)\ntext_transformer = Pipeline(steps=[\n    ('vectorizer', text_vectorizer)\n])\n\n# Update categorical and numerical columns\nnumerical_columns = df_train.select_dtypes('int64').columns\ncategorical_columns = df_train.select_dtypes('object').columns\n\n# Remove target variable from numerical columns\nnumerical_columns = numerical_columns.drop('score')\n\n# Combine transformers using ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_columns),\n        ('cat', categorical_transformer, categorical_columns),\n        ('text', text_transformer, 'clean_text')  # Include the 'clean_text' column\n    ],remainder = 'passthrough')\n\n# Create a pipeline with the preprocessor\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor)])\n\n# Apply the pipeline to your dataset\nX = df_train.drop('score', axis=1)\ny = np.log(df_train['score']) #normalize dependent variable \nX_preprocessed = pipeline.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vectorizer = TfidfVectorizer(\n#     encoding='utf-8',\n#     ngram_range=(1, 3),\n#     strip_accents='unicode',\n#     analyzer='word',\n#     min_df=0.05,\n#     max_df=0.95,\n#     sublinear_tf=True\n# )\n\n# train_vectorized = pd.DataFrame(\n#     vectorizer.fit_transform(df_train['full_text']).toarray(),\n#     columns=[f\"tfidf_{str(f)}\" for f in vectorizer.get_feature_names_out()],\n# )\n\n# train_vectorized.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X = pd.concat([df_train, train_vectorized], axis=1).drop(columns=[\"full_text\", \"score\", 'clean_text'], axis=1)\n# y = df_train[\"score\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, \n                                                    test_size=0.2, random_state=42)\n# Use for vectorize\n# X_train, X_test, y_train, y_test = train_test_split(X, y, \n#                                                     test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\ndef quadratic_weighted_kappa(y_true, y_pred):\n    if isinstance(y_pred, xgb.QuantileDMatrix):\n        # XGB\n        y_true, y_pred = y_pred, y_true\n\n        y_true = (y_true.get_label() + a).round()\n        y_pred = (y_pred + a).clip(1, 6).round()\n        qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n        return 'QWK', qwk\n\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.998\nb = 1.092","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# f1_scores = []\n# kappa_scores = []\n# predictions = []\n# # Define the models\n# models = {\n#     'XGBoost': XGBRegressor(random_state=42)\n# }\n\n# # Define the hyperparameter grids for each model\n# param_grids = {\n#     'XGBoost': {\n#         'n_estimators': [1024],\n#         'learning_rate': [0.1],\n#         'max_depth': [8],\n#         'subsample': [0.5],\n# #         'colsample_bytree': [0.5]\n#     }\n# }\n\n# # 3-fold cross-validation\n# cv = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# # Train and tune the models\n# grids = {}\n# for model_name, model in models.items():\n#     grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n#     grids[model_name].fit(X_train, y_train, eval_metric=quadratic_weighted_kappa)\n#     best_params = grids[model_name].best_params_\n#     best_score = np.sqrt(-1 * grids[model_name].best_score_)\n    \n#     print(f'Best parameters for {model_name}: {best_params}')\n#     print(f'Best RMSE for {model_name}: {best_score}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Khởi tạo danh sách để lưu trữ kết quả\n# kappa_scores = []\n# predictions = []\n\n# # Khởi tạo mô hình với các siêu tham số cố định\n# model = XGBRegressor(\n#     n_estimators=1024,\n#     learning_rate=0.1,\n#     max_depth=8,\n#     subsample=0.5,\n#     random_state=42\n# )\n\n# # 3-fold cross-validation\n# cv = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# # Huấn luyện và đánh giá mô hình\n# for train_index, test_index in cv.split(X_train):\n#     X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n#     y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n    \n#     # Huấn luyện mô hình\n#     model.fit(X_train_fold, y_train_fold)\n    \n#     # Dự đoán trên tập kiểm tra\n#     y_pred = model.predict(X_test_fold)\n    \n#     # Tính toán điểm Kappa\n#     kappa = quadratic_weighted_kappa(y_test_fold, y_pred)\n#     kappa_scores.append(kappa)\n    \n#     # Lưu trữ dự đoán\n#     predictions.extend(y_pred)\n    \n#     print(f'Kappa score for this fold: {kappa}')\n\n# # In kết quả cuối cùng\n# print(f'Average Kappa score: {np.mean(kappa_scores)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Start running...\")\nxgb_regressor = xgb.XGBRegressor(\n    objective=qwk_obj,  # Use custom QWK objective function\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=5,\n    random_state=42\n)\n\n# Train the model\nxgb_regressor.fit(X_train, y_train)\n\n# Make predictions on the validation set\ny_val_pred = xgb_regressor.predict(X_test)\n\n# Convert predictions back to the original scale\ny_val_pred_original = np.exp(y_val_pred)\n\n# Calculate QWK on the validation set\nqwk_score = cohen_kappa_score(y_test.round(), y_val_pred_original.round(), weights=\"quadratic\")\nprint(f\"Validation QWK Score: {qwk_score:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = pd.read_csv('/kaggle/input/test-processed-csv/test_processed.csv')\n# df_test\n\ndf_test = preprocess_df(df_test)\ndf_test['clean_text'] = df_test['full_text'].apply(clean_text)\ndf_test = df_test.drop('full_text', axis = 1)\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Lấy mô hình tốt nhất\n# best_model = grids['XGBoost'].best_estimator_\n# best_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lưu lại cột essay_id để sử dụng sau này\nessay_ids = df_test['essay_id']\n\n# Xóa cột essay_id trước khi tiền xử lý\nX_test = df_test.drop('essay_id', axis=1)\n\n# Áp dụng pipeline đã huấn luyện để tiền xử lý dữ liệu test\nX_test_preprocessed = pipeline.transform(X_test)\n\n# Dự đoán điểm số trên dữ liệu test đã tiền xử lý\ny_pred_log = xgb_regressor.predict(X_test_preprocessed)\n# Chuyển đổi ngược từ log-transform\ny_pred = np.exp(y_pred_log)\n\n# Tạo dataframe chứa essay_id và dự đoán điểm số\nresults = pd.DataFrame({'essay_id': essay_ids, 'score': y_pred})\n\n# Làm tròn điểm số\nresults['score'] = round(results['score'])\nresults['score'] = results['score'].astype(int)\n\n# Lưu kết quả ra file csv\nsubmission = results.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}