{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8404441,"sourceType":"datasetVersion","datasetId":4956698}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Decomment this line of code if you already have those downloaded\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/pyphen-0.15.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/textstat-0.7.3-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:48:38.232668Z","iopub.execute_input":"2024-05-28T08:48:38.233299Z","iopub.status.idle":"2024-05-28T08:48:43.337640Z","shell.execute_reply.started":"2024-05-28T08:48:38.233251Z","shell.execute_reply":"2024-05-28T08:48:43.336275Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/aes-whls/aes_whls/pyphen-0.15.0-py3-none-any.whl\npyphen is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nProcessing /kaggle/input/aes-whls/aes_whls/textstat-0.7.3-py3-none-any.whl\ntextstat is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Basic libary\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport torch\nimport re\nimport optuna\nimport textstat\nfrom optuna.samplers import TPESampler\n# cmap = plt.cm.get_cmap('coolwarm')\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Use for pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import cohen_kappa_score, f1_score, make_scorer\n\n# Use for training model\nfrom scipy.stats import randint\nfrom nltk.tokenize import word_tokenize\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier \nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import RandomizedSearchCV\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-05-28T08:48:43.340890Z","iopub.execute_input":"2024-05-28T08:48:43.341339Z","iopub.status.idle":"2024-05-28T08:48:43.355449Z","shell.execute_reply.started":"2024-05-28T08:48:43.341301Z","shell.execute_reply":"2024-05-28T08:48:43.354159Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\ndf_test = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:48:43.357464Z","iopub.execute_input":"2024-05-28T08:48:43.357910Z","iopub.status.idle":"2024-05-28T08:48:43.803660Z","shell.execute_reply.started":"2024-05-28T08:48:43.357879Z","shell.execute_reply":"2024-05-28T08:48:43.802297Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"essay_id_dropped = df_train['essay_id']\ndf_train = df_train.drop('essay_id', axis = 1)\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:48:43.808317Z","iopub.execute_input":"2024-05-28T08:48:43.808739Z","iopub.status.idle":"2024-05-28T08:48:43.829434Z","shell.execute_reply.started":"2024-05-28T08:48:43.808705Z","shell.execute_reply":"2024-05-28T08:48:43.828135Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 17307 entries, 0 to 17306\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   full_text  17307 non-null  object\n 1   score      17307 non-null  int64 \ndtypes: int64(1), object(1)\nmemory usage: 270.5+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:48:43.830751Z","iopub.execute_input":"2024-05-28T08:48:43.831075Z","iopub.status.idle":"2024-05-28T08:48:43.843894Z","shell.execute_reply.started":"2024-05-28T08:48:43.831047Z","shell.execute_reply":"2024-05-28T08:48:43.842629Z"},"trusted":true},"execution_count":105,"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"                                           full_text  score\n0  Many people have car where they live. The thin...      3\n1  I am a scientist at NASA that is discussing th...      3\n2  People always wish they had the same technolog...      4\n3  We all heard about Venus, the planet without a...      4\n4  Dear, State Senator\\n\\nThis is a letter to arg...      3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Many people have car where they live. The thin...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I am a scientist at NASA that is discussing th...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>People always wish they had the same technolog...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>We all heard about Venus, the planet without a...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\n\ncList = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    # \"he'd\": \"he would\",  ## --> he had or he would\n    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n    # \"I'd\": \"I would\",   ## --> I had or I would\n    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n    # \"it'd\": \"it had\",   ## --> It had or It would\n    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n    # \"she'd\": \"she would\",   ## --> It had or It would\n    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\"so's\": \"so is\",\n    # \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n    # \"there'd\": \"there had\",\n    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n    # \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n    # \"we'd\": \"we had\",\n    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n    \"when's\": \"when is\",\"when've\": \"when have\",\n    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n    \"you're\": \"you are\",  \"you've\": \"you have\"\n}\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef remove_punctuation(text):\n    \"\"\"\n    Remove all punctuation from the input text.\n    \n    Args:\n    - text (str): The input text.\n    \n    Returns:\n    - str: The text with punctuation removed.\n    \"\"\"\n    # string.punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\ndef dataPreprocessing(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = re.sub(r'\\xa0', '', x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef dataPreprocessing_w_contract(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef dataPreprocessing_w_punct_remove(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = remove_punctuation(x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef dataPreprocessing_w_contract_punct_remove(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = remove_punctuation(x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:48:43.846023Z","iopub.execute_input":"2024-05-28T08:48:43.846484Z","iopub.status.idle":"2024-05-28T08:48:43.880976Z","shell.execute_reply.started":"2024-05-28T08:48:43.846449Z","shell.execute_reply":"2024-05-28T08:48:43.879592Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"def textstat_features(text):\n    features = {}\n    features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)\n    features['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(text)\n    features['smog_index'] = textstat.smog_index(text)\n    features['coleman_liau_index'] = textstat.coleman_liau_index(text)\n    features['automated_readability_index'] = textstat.automated_readability_index(text)\n    features['dale_chall_readability_score'] = textstat.dale_chall_readability_score(text)\n    features['difficult_words'] = textstat.difficult_words(text)\n    features['linsear_write_formula'] = textstat.linsear_write_formula(text)\n    features['gunning_fog'] = textstat.gunning_fog(text)\n    features['text_standard'] = textstat.text_standard(text, float_output=True)\n    features['spache_readability'] = textstat.spache_readability(text)\n    features['mcalpine_efg_time'] = textstat.reading_time(text)\n    features['syllablaw'] = textstat.mcalpine_eflaw(text)\n    features['readinle_count'] = textstat.syllable_count(text)\n    features['lexicon_count'] = textstat.lexicon_count(text)\n    features['monosyllabcount'] = textstat.monosyllabcount(text)\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:48:43.882709Z","iopub.execute_input":"2024-05-28T08:48:43.883669Z","iopub.status.idle":"2024-05-28T08:48:43.896343Z","shell.execute_reply.started":"2024-05-28T08:48:43.883624Z","shell.execute_reply":"2024-05-28T08:48:43.895001Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# Number of words\nimport string\ndef preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n    df['word_count'] = df['full_text'].apply(lambda x: len(x.split()))\n\n    # Length\n    df['essay_length'] = df['full_text'].str.len()\n\n    # Sentences count\n    # Adding a new column 'sentences_count' that counts the sentences in 'full_text'\n    df['sentences_count'] = df['full_text'].str.count(r'\\.')\n\n    # Paragraph count\n    # Adding a new column 'paragraph_count' that counts the paragraphs in 'full_text'\n    df['paragraph_count'] = df['full_text'].str.count(r'\\n') + 1\n    \n    df[\"text_tokens\"] = df[\"full_text\"].apply(lambda x: word_tokenize(x))\n    df[\"word_count\"] = df[\"text_tokens\"].apply(lambda x: len(x))\n    df[\"unique_word_count\"] = df[\"text_tokens\"].apply(lambda x: len(set(x)))\n    df.drop(columns=[\"text_tokens\"], inplace=True)\n    \n    df[\"processed_text\"] = df[\"full_text\"].apply(lambda x: dataPreprocessing(x))\n    df[\"text_tokens\"] = df[\"processed_text\"].apply(lambda x: word_tokenize(x))\n    df[\"text_length_p\"] = df[\"processed_text\"].apply(lambda x: len(x))\n    df[\"word_count_p\"] = df[\"text_tokens\"].apply(lambda x: len(x))\n    df[\"unique_word_count_p\"] = df[\"text_tokens\"].apply(lambda x: len(set(x)))\n    \n    # Applying textstat features\n    df['textstat_features'] = df['processed_text'].apply(textstat_features)\n    textstat_df = pd.DataFrame(df['textstat_features'].tolist())\n    df = pd.concat([df, textstat_df], axis=1)\n\n    df.drop(columns=[\"processed_text\", \"text_tokens\", \"textstat_features\"], inplace=True)\n    \n    return df\n\ndef clean_text(text):\n    text = re.sub(r'\\n', ' ', text)  # Loại bỏ xuống dòng\n    text = re.sub(r'[^\\w\\s]', '', text)  # Loại bỏ ký tự đặc biệt\n    text = re.sub(r'\\xa0', '', text)\n    text = text.lower()  # Chuyển thành chữ thường\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:48:43.897651Z","iopub.execute_input":"2024-05-28T08:48:43.898095Z","iopub.status.idle":"2024-05-28T08:48:43.915491Z","shell.execute_reply.started":"2024-05-28T08:48:43.898054Z","shell.execute_reply":"2024-05-28T08:48:43.914486Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"df_train = preprocess_df(df_train)\ndf_train['clean_text'] = df_train['full_text'].apply(clean_text)\ndf_train = df_train.drop('full_text', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:48:43.917492Z","iopub.execute_input":"2024-05-28T08:48:43.917964Z","iopub.status.idle":"2024-05-28T08:54:03.957649Z","shell.execute_reply.started":"2024-05-28T08:48:43.917921Z","shell.execute_reply":"2024-05-28T08:54:03.956319Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:54:03.962326Z","iopub.execute_input":"2024-05-28T08:54:03.962707Z","iopub.status.idle":"2024-05-28T08:54:03.987760Z","shell.execute_reply.started":"2024-05-28T08:54:03.962679Z","shell.execute_reply":"2024-05-28T08:54:03.986490Z"},"trusted":true},"execution_count":110,"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"   score  word_count  essay_length  sentences_count  paragraph_count  \\\n0      3         545          2677               13                1   \n1      3         371          1669               19                9   \n2      4         605          3077               24                7   \n3      4         511          2701               23                9   \n4      3         418          2208               15               11   \n\n   unique_word_count  text_length_p  word_count_p  unique_word_count_p  \\\n0                248           2640           539                  227   \n1                168           1663           371                  152   \n2                243           3065           605                  231   \n3                241           2674           502                  223   \n4                156           2184           417                  148   \n\n   flesch_reading_ease  ...  linsear_write_formula  gunning_fog  \\\n0                58.69  ...              13.000000        17.08   \n1                87.55  ...               6.714286         7.48   \n2                65.15  ...              15.500000        11.49   \n3                58.62  ...              15.750000        11.85   \n4                54.76  ...              19.666667        12.61   \n\n   text_standard  spache_readability  mcalpine_efg_time  syllablaw  \\\n0           12.0                7.20              31.58       53.4   \n1            7.0                3.92              19.57       25.7   \n2           12.0                5.12              36.96       32.6   \n3           13.0                5.32              32.74       28.9   \n4           13.0                5.61              26.62       35.6   \n\n   readinle_count  lexicon_count  monosyllabcount  \\\n0             624            489              396   \n1             398            332              275   \n2             767            550              417   \n3             678            441              284   \n4             561            372              240   \n\n                                          clean_text  \n0  many people have car where they live the thing...  \n1  i am a scientist at nasa that is discussing th...  \n2  people always wish they had the same technolog...  \n3  we all heard about venus the planet without al...  \n4  dear state senator  this is a letter to argue ...  \n\n[5 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>word_count</th>\n      <th>essay_length</th>\n      <th>sentences_count</th>\n      <th>paragraph_count</th>\n      <th>unique_word_count</th>\n      <th>text_length_p</th>\n      <th>word_count_p</th>\n      <th>unique_word_count_p</th>\n      <th>flesch_reading_ease</th>\n      <th>...</th>\n      <th>linsear_write_formula</th>\n      <th>gunning_fog</th>\n      <th>text_standard</th>\n      <th>spache_readability</th>\n      <th>mcalpine_efg_time</th>\n      <th>syllablaw</th>\n      <th>readinle_count</th>\n      <th>lexicon_count</th>\n      <th>monosyllabcount</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>545</td>\n      <td>2677</td>\n      <td>13</td>\n      <td>1</td>\n      <td>248</td>\n      <td>2640</td>\n      <td>539</td>\n      <td>227</td>\n      <td>58.69</td>\n      <td>...</td>\n      <td>13.000000</td>\n      <td>17.08</td>\n      <td>12.0</td>\n      <td>7.20</td>\n      <td>31.58</td>\n      <td>53.4</td>\n      <td>624</td>\n      <td>489</td>\n      <td>396</td>\n      <td>many people have car where they live the thing...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>371</td>\n      <td>1669</td>\n      <td>19</td>\n      <td>9</td>\n      <td>168</td>\n      <td>1663</td>\n      <td>371</td>\n      <td>152</td>\n      <td>87.55</td>\n      <td>...</td>\n      <td>6.714286</td>\n      <td>7.48</td>\n      <td>7.0</td>\n      <td>3.92</td>\n      <td>19.57</td>\n      <td>25.7</td>\n      <td>398</td>\n      <td>332</td>\n      <td>275</td>\n      <td>i am a scientist at nasa that is discussing th...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>605</td>\n      <td>3077</td>\n      <td>24</td>\n      <td>7</td>\n      <td>243</td>\n      <td>3065</td>\n      <td>605</td>\n      <td>231</td>\n      <td>65.15</td>\n      <td>...</td>\n      <td>15.500000</td>\n      <td>11.49</td>\n      <td>12.0</td>\n      <td>5.12</td>\n      <td>36.96</td>\n      <td>32.6</td>\n      <td>767</td>\n      <td>550</td>\n      <td>417</td>\n      <td>people always wish they had the same technolog...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>511</td>\n      <td>2701</td>\n      <td>23</td>\n      <td>9</td>\n      <td>241</td>\n      <td>2674</td>\n      <td>502</td>\n      <td>223</td>\n      <td>58.62</td>\n      <td>...</td>\n      <td>15.750000</td>\n      <td>11.85</td>\n      <td>13.0</td>\n      <td>5.32</td>\n      <td>32.74</td>\n      <td>28.9</td>\n      <td>678</td>\n      <td>441</td>\n      <td>284</td>\n      <td>we all heard about venus the planet without al...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>418</td>\n      <td>2208</td>\n      <td>15</td>\n      <td>11</td>\n      <td>156</td>\n      <td>2184</td>\n      <td>417</td>\n      <td>148</td>\n      <td>54.76</td>\n      <td>...</td>\n      <td>19.666667</td>\n      <td>12.61</td>\n      <td>13.0</td>\n      <td>5.61</td>\n      <td>26.62</td>\n      <td>35.6</td>\n      <td>561</td>\n      <td>372</td>\n      <td>240</td>\n      <td>dear state senator  this is a letter to argue ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 26 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:54:03.989397Z","iopub.execute_input":"2024-05-28T08:54:03.989744Z","iopub.status.idle":"2024-05-28T08:54:04.015540Z","shell.execute_reply.started":"2024-05-28T08:54:03.989715Z","shell.execute_reply":"2024-05-28T08:54:04.014132Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 17307 entries, 0 to 17306\nData columns (total 26 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   score                         17307 non-null  int64  \n 1   word_count                    17307 non-null  int64  \n 2   essay_length                  17307 non-null  int64  \n 3   sentences_count               17307 non-null  int64  \n 4   paragraph_count               17307 non-null  int64  \n 5   unique_word_count             17307 non-null  int64  \n 6   text_length_p                 17307 non-null  int64  \n 7   word_count_p                  17307 non-null  int64  \n 8   unique_word_count_p           17307 non-null  int64  \n 9   flesch_reading_ease           17307 non-null  float64\n 10  flesch_kincaid_grade          17307 non-null  float64\n 11  smog_index                    17307 non-null  float64\n 12  coleman_liau_index            17307 non-null  float64\n 13  automated_readability_index   17307 non-null  float64\n 14  dale_chall_readability_score  17307 non-null  float64\n 15  difficult_words               17307 non-null  int64  \n 16  linsear_write_formula         17307 non-null  float64\n 17  gunning_fog                   17307 non-null  float64\n 18  text_standard                 17307 non-null  float64\n 19  spache_readability            17307 non-null  float64\n 20  mcalpine_efg_time             17307 non-null  float64\n 21  syllablaw                     17307 non-null  float64\n 22  readinle_count                17307 non-null  int64  \n 23  lexicon_count                 17307 non-null  int64  \n 24  monosyllabcount               17307 non-null  int64  \n 25  clean_text                    17307 non-null  object \ndtypes: float64(12), int64(13), object(1)\nmemory usage: 3.4+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# rows_to_drop = df_train.query('sentences_count > 60 | (sentences_count > 50 & score == 1)').index\n\n# # Xóa các dòng có index tương ứng\n\n# df_train.drop(rows_to_drop, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:54:04.017004Z","iopub.execute_input":"2024-05-28T08:54:04.017367Z","iopub.status.idle":"2024-05-28T08:54:04.022411Z","shell.execute_reply.started":"2024-05-28T08:54:04.017336Z","shell.execute_reply":"2024-05-28T08:54:04.021224Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"# rows_to_drop = df_train.query('paragraph_count > 80 | (paragraph_count > 55 & score == 1 ) | (paragraph_count >60 & score == 2 ) | (paragraph_count > 30  & score == 5) | (paragraph_count > 25  & score == 6)').index\n\n# # Xóa các dòng có index tương ứng\n\n# df_train.drop(rows_to_drop, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:54:04.024039Z","iopub.execute_input":"2024-05-28T08:54:04.024416Z","iopub.status.idle":"2024-05-28T08:54:04.034574Z","shell.execute_reply.started":"2024-05-28T08:54:04.024385Z","shell.execute_reply":"2024-05-28T08:54:04.033522Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"# Decomment if want to use Pipeline again\nprint(\"Start running...\")\n# Define transformers for numerical and categorical columns\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse = False))\n])\n\n# Add text vectorization step\ntext_vectorizer = TfidfVectorizer(\n    encoding='utf-8',\n    ngram_range=(1, 3),\n    strip_accents='unicode',\n    analyzer='word',\n    min_df=0.05,\n    max_df=0.95,\n    sublinear_tf=True\n)\ntext_transformer = Pipeline(steps=[\n    ('vectorizer', text_vectorizer)\n])\n\n# Update categorical and numerical columns\nnumerical_columns = df_train.select_dtypes('int64').columns\ncategorical_columns = df_train.select_dtypes('object').columns\n\n# Remove target variable from numerical columns\nnumerical_columns = numerical_columns.drop('score')\n\n# Combine transformers using ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_columns),\n        ('cat', categorical_transformer, categorical_columns),\n        ('text', text_transformer, 'clean_text')  # Include the 'clean_text' column\n    ],remainder = 'passthrough')\n\n# Create a pipeline with the preprocessor\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor)])\n\n# Apply the pipeline to your dataset\nX = df_train.drop('score', axis=1)\ny = np.log(df_train['score']) #normalize dependent variable \nX_preprocessed = pipeline.fit_transform(X)\nprint(\"This Pipeline is done\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:54:04.036090Z","iopub.execute_input":"2024-05-28T08:54:04.036556Z","iopub.status.idle":"2024-05-28T08:54:38.593687Z","shell.execute_reply.started":"2024-05-28T08:54:04.036516Z","shell.execute_reply":"2024-05-28T08:54:38.592512Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"Start running...\nThis Pipeline is done\n","output_type":"stream"}]},{"cell_type":"code","source":"# vectorizer = TfidfVectorizer(\n#     encoding='utf-8',\n#     ngram_range=(1, 3),\n#     strip_accents='unicode',\n#     analyzer='word',\n#     min_df=0.05,\n#     max_df=0.95,\n#     sublinear_tf=True\n# )\n\n# train_vectorized = pd.DataFrame(\n#     vectorizer.fit_transform(df_train['full_text']).toarray(),\n#     columns=[f\"tfidf_{str(f)}\" for f in vectorizer.get_feature_names_out()],\n# )\n\n# train_vectorized.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:54:38.595463Z","iopub.execute_input":"2024-05-28T08:54:38.596154Z","iopub.status.idle":"2024-05-28T08:54:38.601955Z","shell.execute_reply.started":"2024-05-28T08:54:38.596113Z","shell.execute_reply":"2024-05-28T08:54:38.600504Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"# X = pd.concat([df_train, train_vectorized], axis=1).drop(columns=[\"full_text\", \"score\", 'clean_text'], axis=1)\n# y = df_train[\"score\"]","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:54:38.603848Z","iopub.execute_input":"2024-05-28T08:54:38.604657Z","iopub.status.idle":"2024-05-28T08:54:38.620229Z","shell.execute_reply.started":"2024-05-28T08:54:38.604623Z","shell.execute_reply":"2024-05-28T08:54:38.619136Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, \n                                                    test_size=0.2, random_state=42)\n# Use for vectorize\n# X_train, X_test, y_train, y_test = train_test_split(X, y, \n#                                                     test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:54:38.624238Z","iopub.execute_input":"2024-05-28T08:54:38.624638Z","iopub.status.idle":"2024-05-28T08:54:39.774486Z","shell.execute_reply.started":"2024-05-28T08:54:38.624607Z","shell.execute_reply":"2024-05-28T08:54:39.773440Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\n\n# metric and objective based on public notebooks\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.998\nb = 1.092","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:54:39.775682Z","iopub.execute_input":"2024-05-28T08:54:39.775988Z","iopub.status.idle":"2024-05-28T08:54:39.786535Z","shell.execute_reply.started":"2024-05-28T08:54:39.775961Z","shell.execute_reply":"2024-05-28T08:54:39.785227Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"# f1_scores = []\n# kappa_scores = []\n# predictions = []\n# # Define the models\n# models = {\n#     'XGBoost': XGBRegressor(random_state=42)\n# }\n\n# # Define the hyperparameter grids for each model\n# param_grids = {\n#     'XGBoost': {\n#         'n_estimators': [1024],\n#         'learning_rate': [0.1],\n#         'max_depth': [8],\n#         'subsample': [0.5],\n# #         'colsample_bytree': [0.5]\n#     }\n# }\n\n# # 3-fold cross-validation\n# cv = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# # Train and tune the models\n# grids = {}\n# for model_name, model in models.items():\n#     grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], \n#                                cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n#     grids[model_name].fit(X_train, y_train, eval_metric=quadratic_weighted_kappa)\n#     best_params = grids[model_name].best_params_\n#     best_score = np.sqrt(-1 * grids[model_name].best_score_)\n    \n#     print(f'Best parameters for {model_name}: {best_params}')\n#     print(f'Best RMSE for {model_name}: {best_score}\\n')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:54:39.788089Z","iopub.execute_input":"2024-05-28T08:54:39.788543Z","iopub.status.idle":"2024-05-28T08:54:39.797551Z","shell.execute_reply.started":"2024-05-28T08:54:39.788503Z","shell.execute_reply":"2024-05-28T08:54:39.796511Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"print(\"Start running...\")\n\n# # Define the hyperparameter grids for each model\n# param_grids = {\n#     \"n_estimators\": randint(100, 500),\n#     \"learning_rate\": round(np.random.random(),1),\n#     \"max_depth\": randint(1, 9),\n#     \"subsample\": round(np.random.random(),1),\n#     \"max_features\": randint(1, 9),\n# }\n\nxgb_regressor = xgb.XGBRegressor(\n    objective=qwk_obj,  # Use custom QWK objective function\n    n_estimators= 100,\n    learning_rate= 0.1,\n    max_depth= 8,\n    subsample= 0.6154974601038354,\n    random_state=42\n)\n\n# Train the model\nxgb_regressor.fit(X_train, y_train,\n                 eval_metric=quadratic_weighted_kappa)\n\n# Make predictions on the validation set\ny_pred = xgb_regressor.predict(X_test)\n\n# Convert predictions back to the original scale\ny_pred = (y_pred + a).clip(1, 6).round()\n\n# Calculate QWK on the validation set\nqwk_score = cohen_kappa_score(y_test, y_pred, weights=\"quadratic\")\nprint(f\"Validation QWK Score: {qwk_score:.4f}\")\n\nscore = quadratic_weighted_kappa(y_test, y_pred)\nprint(f\"Train QWK: {score}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:34:46.950115Z","iopub.execute_input":"2024-05-28T09:34:46.950660Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Start running...\n","output_type":"stream"}]},{"cell_type":"code","source":"def objective(trial):\n    param_grid = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 1, 9),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n    }\n\n    xgb_regressor = xgb.XGBRegressor(\n        objective=qwk_obj,\n        random_state=42,\n        **param_grid\n    )\n    \n    xgb_regressor.fit(X_train, y_train)\n    y_val_pred = xgb_regressor.predict(X_test)\n    y_val_pred_original = np.exp(y_val_pred)\n    qwk_score = quadratic_weighted_kappa(y_test.round(), y_val_pred_original.round())\n    return np.mean(qwk_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:56:33.306982Z","iopub.status.idle":"2024-05-28T08:56:33.307602Z","shell.execute_reply.started":"2024-05-28T08:56:33.307291Z","shell.execute_reply":"2024-05-28T08:56:33.307314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up the Optuna study\nstudy = optuna.create_study(direction='maximize', sampler=TPESampler())\nstudy.optimize(objective, n_trials=10, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:56:33.308930Z","iopub.status.idle":"2024-05-28T08:56:33.309482Z","shell.execute_reply.started":"2024-05-28T08:56:33.309193Z","shell.execute_reply":"2024-05-28T08:56:33.309234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best hyperparameters\nbest_params = study.best_params\nprint(\"Best Hyperparameters:\", best_params)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:56:33.310700Z","iopub.status.idle":"2024-05-28T08:56:33.311077Z","shell.execute_reply.started":"2024-05-28T08:56:33.310897Z","shell.execute_reply":"2024-05-28T08:56:33.310912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the final model with the best hyperparameters\nxgb_regressor = xgb.XGBRegressor(\n    objective=qwk_obj,\n    random_state=42,\n    **best_params\n)\n\nxgb_regressor.fit(\n    X_train, y_train,\n    early_stopping_rounds=10,\n    eval_set=[(X_test, y_test)],\n    verbose=True\n)\n\ny_val_pred = xgb_regressor.predict(X_test)\ny_val_pred_original = np.exp(y_val_pred)\nqwk_score = cohen_kappa_score(y_test.round(), y_val_pred_original.round(), weights=\"quadratic\")\nprint(f\"Validation QWK Score: {qwk_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:56:33.313194Z","iopub.status.idle":"2024-05-28T08:56:33.313763Z","shell.execute_reply.started":"2024-05-28T08:56:33.313463Z","shell.execute_reply":"2024-05-28T08:56:33.313488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = pd.read_csv('/kaggle/input/test-processed-csv/test_processed.csv')\n# df_test\n\ndf_test = preprocess_df(df_test)\ndf_test['clean_text'] = df_test['full_text'].apply(clean_text)\ndf_test = df_test.drop('full_text', axis = 1)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:56:33.315827Z","iopub.status.idle":"2024-05-28T08:56:33.316513Z","shell.execute_reply.started":"2024-05-28T08:56:33.316284Z","shell.execute_reply":"2024-05-28T08:56:33.316302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Lấy mô hình tốt nhất\n# best_model = grids['XGBoost'].best_estimator_\n# best_model","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:56:33.317816Z","iopub.status.idle":"2024-05-28T08:56:33.318530Z","shell.execute_reply.started":"2024-05-28T08:56:33.318300Z","shell.execute_reply":"2024-05-28T08:56:33.318318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lưu lại cột essay_id để sử dụng sau này\nessay_ids = df_test['essay_id']\n\n# Xóa cột essay_id trước khi tiền xử lý\nX_test = df_test.drop('essay_id', axis=1)\n\n# Áp dụng pipeline đã huấn luyện để tiền xử lý dữ liệu test\nX_test_preprocessed = pipeline.transform(X_test)\n\n# Dự đoán điểm số trên dữ liệu test đã tiền xử lý\ny_pred_log = xgb_regressor.predict(X_test_preprocessed)\n# Chuyển đổi ngược từ log-transform\ny_pred = np.exp(y_pred_log)\n\n# Tạo dataframe chứa essay_id và dự đoán điểm số\nresults = pd.DataFrame({'essay_id': essay_ids, 'score': y_pred})\n\n# Làm tròn điểm số\nresults['score'] = round(results['score'])\nresults['score'] = results['score'].astype(int)\n\n# Lưu kết quả ra file csv\nsubmission = results.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:56:33.319843Z","iopub.status.idle":"2024-05-28T08:56:33.320541Z","shell.execute_reply.started":"2024-05-28T08:56:33.320331Z","shell.execute_reply":"2024-05-28T08:56:33.320349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:56:33.321806Z","iopub.status.idle":"2024-05-28T08:56:33.322496Z","shell.execute_reply.started":"2024-05-28T08:56:33.322264Z","shell.execute_reply":"2024-05-28T08:56:33.322282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}